{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de2c258d-2aa4-4111-888b-b8512a8c8fc4",
   "metadata": {},
   "source": [
    "# RTML | A2 Object Detection\n",
    "\n",
    "-----------\n",
    "### Ulugbek Shernazarov - st125457\n",
    "\n",
    "### Object Detection\n",
    "\n",
    "If you could go back in time to the 1990s, there were no cameras that could find faces in a photograph, and no\n",
    "researcher had a way to count dogs in a video in real time. Everyone had to count the dogs manually.\n",
    "Times were very tough.\n",
    "\n",
    "The Holy Grail of computer vision research at the time was real time face detection. If we could find faces\n",
    "in images fast enough, we could build systems that interact more naturally with human beings. But nobody had\n",
    "a solution.\n",
    "\n",
    "Things changed when Viola and Jones introduced the first real time face detector, the Haar-like cascade, at\n",
    "the end of the 1990s.\n",
    "This technique swept a detection window over the input image at multiple sizes, and subjected each local patch\n",
    "to a cascade of simple rough classifiers. Each patch that made it to the end of the cascade of classifiers was\n",
    "treated as a positive detection. After a set of candidate patches were identified, there would be a cleanup\n",
    "stage when neighboring detections are clustered into isolated detections.\n",
    "\n",
    "This method and one cousin, the HOG detector, which was slower but a little more accurate, dominated during the 2000s\n",
    "and on into the 2010s. These methods worked well enough when trained carefully on the specific environment they were\n",
    "used in, but usually couldn't be transfer to a new environment.\n",
    "\n",
    "With the introduction of AlexNet and the amazing advances in image classification, we could follow the direction\n",
    "of R-CNN, to use a region proposal algorithm followed by a deep learning classifier to do object detection VERY slowly\n",
    "but much more accurately than the old real time methods.\n",
    "\n",
    "\n",
    "### Task 1: Inference\n",
    "\n",
    "In the lab, we saw how the Darknet configuration file for YOLOv3 could be read in Python and mapped\n",
    "to PyTorch modules.\n",
    "\n",
    "For your independent work do the same thing for YOLOv4. Download the `yolov4.cfg` file\n",
    "from the [YOLOv4 GitHub repository](https://github.com/AlexeyAB/darknet) and modify your\n",
    "`MyDarknet` class and utility code (`darknet.py`, `util.py`) as\n",
    "necessary to map the structures to PyTorch.\n",
    "\n",
    "The changes you'll have to make:\n",
    "\n",
    "1. Implement the mish activation function - Done\n",
    "2. Add an option for a maxpool layer in the `create_modules` function and in your model's `forward()` method. - Done\n",
    "3. Enable a `[route]` module to concatenate more than two previous layers - Done\n",
    "4. Load the pre-trained weights [provided by the authors](https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights) - Done\n",
    "4. Scale inputs to 608$\\times$608 and make sure you're passing input channels in RGB order, not OpenCV's BGR order. - Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61712133-c199-4629-b195-5a3de5ad224b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mish.py\n",
    "# 1. Implementation of the mish activation function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    \"\"\"\n",
    "    Mish activation function implementation:\n",
    "        mish = x * tanh(softplus(x)) = x * tanh(ln(1 + exp(x)))\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Mish, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.tanh(F.softplus(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a713646f-20e0-46ef-92a4-c14fcb50392e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# darknet.py\n",
    "# 2. Add an option for a maxpool layer in the `create_modules` function and in your model's `forward()` method.\n",
    "# 3. Enable a `[route]` module to concatenate more than two previous layers\n",
    "from __future__ import division\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from utils.mish import Mish\n",
    "from utils.util import * \n",
    "\n",
    "\n",
    "\n",
    "def get_test_input():\n",
    "    img = cv2.imread(\"dog-cycle-car.png\")\n",
    "    img = cv2.resize(img, (608,608))          #Resize to the input dimension\n",
    "    img_ =  img[:,:,::-1].transpose((2,0,1))  # BGR -> RGB | H X W C -> C X H X W \n",
    "    img_ = img_[np.newaxis,:,:,:]/255.0       #Add a channel at 0 (for batch) | Normalise\n",
    "    img_ = torch.from_numpy(img_).float()     #Convert to float\n",
    "    img_ = Variable(img_)                     # Convert to Variable\n",
    "    return img_\n",
    "\n",
    "def parse_cfg(cfgfile):\n",
    "    \"\"\"\n",
    "    Takes a configuration file\n",
    "    \n",
    "    Returns a list of blocks. Each blocks describes a block in the neural\n",
    "    network to be built. Block is represented as a dictionary in the list\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    file = open(cfgfile, 'r')\n",
    "    lines = file.read().split('\\n')                        # store the lines in a list\n",
    "    lines = [x for x in lines if len(x) > 0]               # get read of the empty lines \n",
    "    lines = [x for x in lines if x[0] != '#']              # get rid of comments\n",
    "    lines = [x.rstrip().lstrip() for x in lines]           # get rid of fringe whitespaces\n",
    "    \n",
    "    block = {}\n",
    "    blocks = []\n",
    "    \n",
    "    for line in lines:\n",
    "        if line[0] == \"[\":               # This marks the start of a new block\n",
    "            if len(block) != 0:          # If block is not empty, implies it is storing values of previous block.\n",
    "                blocks.append(block)     # add it the blocks list\n",
    "                block = {}               # re-init the block\n",
    "            block[\"type\"] = line[1:-1].rstrip()     \n",
    "        else:\n",
    "            key,value = line.split(\"=\") \n",
    "            block[key.rstrip()] = value.lstrip()\n",
    "    blocks.append(block)\n",
    "    \n",
    "    return blocks\n",
    "\n",
    "def get_anchors(blocks):\n",
    "    \"\"\"Extract anchors from YOLO layers in config blocks\"\"\"\n",
    "    anchors = []\n",
    "    for block in blocks:\n",
    "        if block[\"type\"] == \"yolo\":\n",
    "            if \"anchors\" in block:\n",
    "                # Parse anchor string into list of tuples\n",
    "                anchor_pairs = [float(x) for x in block[\"anchors\"].split(\",\")]\n",
    "                block_anchors = [(anchor_pairs[i], anchor_pairs[i+1]) for i in range(0, len(anchor_pairs), 2)]\n",
    "                anchors.extend(block_anchors)\n",
    "    return anchors\n",
    "\n",
    "def prep_image(img, inp_dim):\n",
    "    \"\"\"\n",
    "    Prepare image for inputting to the neural network. Returns a tensor.\n",
    "    \"\"\"\n",
    "    # pylint: disable=no-member\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = letterbox_image(img, (inp_dim, inp_dim))\n",
    "    return torch.from_numpy(img.transpose(2, 0, 1)).float().div(255.0).unsqueeze(0)\n",
    "\n",
    "\n",
    "class EmptyLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmptyLayer, self).__init__()\n",
    "        \n",
    "\n",
    "class DetectionLayer(nn.Module):\n",
    "    def __init__(self, anchors):\n",
    "        super(DetectionLayer, self).__init__()\n",
    "        self.anchors = anchors\n",
    "\n",
    "\n",
    "\n",
    "def create_modules(blocks):\n",
    "    net_info = blocks[0]     #Captures the information about the input and pre-processing    \n",
    "    module_list = nn.ModuleList()\n",
    "    prev_filters = 3\n",
    "    output_filters = []\n",
    "    \n",
    "    for index, x in enumerate(blocks[1:]):\n",
    "        module = nn.Sequential()\n",
    "    \n",
    "        #check the type of block\n",
    "        #create a new module for the block\n",
    "        #append to module_list\n",
    "        \n",
    "        #If it's a convolutional layer\n",
    "        if (x[\"type\"] == \"convolutional\"):\n",
    "            #Get the info about the layer\n",
    "            activation = x[\"activation\"]\n",
    "            try:\n",
    "                batch_normalize = int(x[\"batch_normalize\"])\n",
    "                bias = False\n",
    "            except:\n",
    "                batch_normalize = 0\n",
    "                bias = True\n",
    "        \n",
    "            filters= int(x[\"filters\"])\n",
    "            padding = int(x[\"pad\"])\n",
    "            kernel_size = int(x[\"size\"])\n",
    "            stride = int(x[\"stride\"])\n",
    "        \n",
    "            if padding:\n",
    "                pad = (kernel_size - 1) // 2\n",
    "            else:\n",
    "                pad = 0\n",
    "        \n",
    "            #Add the convolutional layer\n",
    "            conv = nn.Conv2d(prev_filters, filters, kernel_size, stride, pad, bias = bias)\n",
    "            module.add_module(\"conv_{0}\".format(index), conv)\n",
    "        \n",
    "            #Add the Batch Norm Layer\n",
    "            if batch_normalize:\n",
    "                bn = nn.BatchNorm2d(filters)\n",
    "                module.add_module(\"batch_norm_{0}\".format(index), bn)\n",
    "        \n",
    "            #Check the activation. \n",
    "            #It is either Linear or a Leaky ReLU for YOLO\n",
    "            if activation == \"leaky\":\n",
    "                activn = nn.LeakyReLU(0.1, inplace = True)\n",
    "                module.add_module(\"leaky_{0}\".format(index), activn)\n",
    "                \n",
    "            elif activation == \"mish\":\n",
    "                activn = Mish()\n",
    "                module.add_module(\"mish_{0}\".format(index), activn)\n",
    "\n",
    "        \n",
    "            #If it's an upsampling layer\n",
    "            #We use Bilinear2dUpsampling\n",
    "        elif (x[\"type\"] == \"upsample\"):\n",
    "            stride = int(x[\"stride\"])\n",
    "            upsample = nn.Upsample(scale_factor = 2, mode = \"nearest\")\n",
    "            module.add_module(\"upsample_{}\".format(index), upsample)\n",
    "                \n",
    "        #If it is a route layer\n",
    "        elif (x[\"type\"] == \"route\"):\n",
    "            x[\"layers\"] = x[\"layers\"].split(',')\n",
    "            filters = 0\n",
    "\n",
    "            for i in range(len(x[\"layers\"])):\n",
    "                pointer = int(x[\"layers\"][i])\n",
    "                if  pointer > 0:\n",
    "                    filters += output_filters[pointer]\n",
    "                else:\n",
    "                    filters += output_filters[index + pointer]\n",
    "\n",
    "            route = EmptyLayer()\n",
    "            module.add_module(\"route_{0}\".format(index), route)\n",
    "    \n",
    "        #shortcut corresponds to skip connection\n",
    "        elif x[\"type\"] == \"shortcut\":\n",
    "            shortcut = EmptyLayer()\n",
    "            module.add_module(\"shortcut_{}\".format(index), shortcut)\n",
    "            \n",
    "        #Yolo is the detection layer\n",
    "        elif x[\"type\"] == \"yolo\":\n",
    "            mask = x[\"mask\"].split(\",\")\n",
    "            mask = [int(x) for x in mask]\n",
    "    \n",
    "            anchors = x[\"anchors\"].split(\",\")\n",
    "            anchors = [int(a) for a in anchors]\n",
    "            anchors = [(anchors[i], anchors[i+1]) for i in range(0, len(anchors),2)]\n",
    "            anchors = [anchors[i] for i in mask]\n",
    "    \n",
    "            detection = DetectionLayer(anchors)\n",
    "            module.add_module(\"Detection_{}\".format(index), detection)\n",
    "        \n",
    "        # Max pooling layer\n",
    "        elif x[\"type\"] == \"maxpool\":\n",
    "            stride = int(x[\"stride\"])\n",
    "            size = int(x[\"size\"])\n",
    "            max_pool = nn.MaxPool2d(size, stride, padding=size // 2)\n",
    "            module.add_module(\"maxpool_{}\".format(index), max_pool)\n",
    "                              \n",
    "        module_list.append(module)\n",
    "        prev_filters = filters\n",
    "        output_filters.append(filters)\n",
    "        \n",
    "    return (net_info, module_list)\n",
    "\n",
    "class Darknet(nn.Module):\n",
    "    def __init__(self, cfgfile):\n",
    "        super(Darknet, self).__init__()\n",
    "        self.blocks = parse_cfg(cfgfile)\n",
    "        self.net_info, self.module_list = create_modules(self.blocks)\n",
    "        self.anchors = get_anchors(self.blocks)  # Parse and store anchors\n",
    "\n",
    "    def forward(self, x, CUDA):\n",
    "        modules = self.blocks[1:]\n",
    "        outputs = {}   #We cache the outputs for the route layer\n",
    "        \n",
    "        write = 0\n",
    "        for i, module in enumerate(modules):        \n",
    "            module_type = (module[\"type\"])\n",
    "            \n",
    "\n",
    "            if module_type in [\"convolutional\", \"upsample\", \"maxpool\"]:\n",
    "                x = self.module_list[i](x)\n",
    "            \n",
    "            elif module_type == \"route\":\n",
    "                layers = module[\"layers\"]\n",
    "                layers = [int(a) for a in layers]\n",
    "                maps = []\n",
    "                for l in range(0, len(layers)):\n",
    "                    if layers[l] > 0:\n",
    "                        layers[l] = layers[l] - i\n",
    "                    maps.append(outputs[i + layers[l]])\n",
    "                x = torch.cat((maps), 1)\n",
    "                \n",
    "    \n",
    "            elif  module_type == \"shortcut\":\n",
    "                from_ = int(module[\"from\"])\n",
    "                x = outputs[i-1] + outputs[i+from_]\n",
    "    \n",
    "            elif module_type == 'yolo':        \n",
    "                anchors = self.module_list[i][0].anchors\n",
    "                #Get the input dimensions\n",
    "                inp_dim = int (self.net_info[\"height\"])\n",
    "        \n",
    "                #Get the number of classes\n",
    "                num_classes = int (module[\"classes\"])\n",
    "        \n",
    "                #Transform \n",
    "                x = x.data\n",
    "                x = predict_transform(x, inp_dim, anchors, num_classes, CUDA)\n",
    "                if not write:              #if no collector has been intialised. \n",
    "                    detections = x\n",
    "                    write = 1\n",
    "        \n",
    "                else:       \n",
    "                    detections = torch.cat((detections, x), 1)\n",
    "        \n",
    "            outputs[i] = x\n",
    "        \n",
    "        return detections\n",
    "\n",
    "\n",
    "    def load_weights(self, weightfile):\n",
    "        #Open the weights file\n",
    "        fp = open(weightfile, \"rb\")\n",
    "    \n",
    "        #The first 5 values are header information \n",
    "        # 1. Major version number\n",
    "        # 2. Minor Version Number\n",
    "        # 3. Subversion number \n",
    "        # 4,5. Images seen by the network (during training)\n",
    "        header = np.fromfile(fp, dtype = np.int32, count = 5)\n",
    "        self.header = torch.from_numpy(header)\n",
    "        self.seen = self.header[3]   \n",
    "        \n",
    "        weights = np.fromfile(fp, dtype = np.float32)\n",
    "        \n",
    "        ptr = 0\n",
    "        for i in range(len(self.module_list)):\n",
    "            module_type = self.blocks[i + 1][\"type\"]\n",
    "    \n",
    "            #If module_type is convolutional load weights\n",
    "            #Otherwise ignore.\n",
    "            \n",
    "            if module_type == \"convolutional\":\n",
    "                model = self.module_list[i]\n",
    "                try:\n",
    "                    batch_normalize = int(self.blocks[i+1][\"batch_normalize\"])\n",
    "                except:\n",
    "                    batch_normalize = 0\n",
    "            \n",
    "                conv = model[0]\n",
    "                \n",
    "                \n",
    "                if (batch_normalize):\n",
    "                    bn = model[1]\n",
    "        \n",
    "                    #Get the number of weights of Batch Norm Layer\n",
    "                    num_bn_biases = bn.bias.numel()\n",
    "        \n",
    "                    #Load the weights\n",
    "                    bn_biases = torch.from_numpy(weights[ptr:ptr + num_bn_biases])\n",
    "                    ptr += num_bn_biases\n",
    "        \n",
    "                    bn_weights = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n",
    "                    ptr  += num_bn_biases\n",
    "        \n",
    "                    bn_running_mean = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n",
    "                    ptr  += num_bn_biases\n",
    "        \n",
    "                    bn_running_var = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n",
    "                    ptr  += num_bn_biases\n",
    "        \n",
    "                    #Cast the loaded weights into dims of model weights. \n",
    "                    bn_biases = bn_biases.view_as(bn.bias.data)\n",
    "                    bn_weights = bn_weights.view_as(bn.weight.data)\n",
    "                    bn_running_mean = bn_running_mean.view_as(bn.running_mean)\n",
    "                    bn_running_var = bn_running_var.view_as(bn.running_var)\n",
    "        \n",
    "                    #Copy the data to model\n",
    "                    bn.bias.data.copy_(bn_biases)\n",
    "                    bn.weight.data.copy_(bn_weights)\n",
    "                    bn.running_mean.copy_(bn_running_mean)\n",
    "                    bn.running_var.copy_(bn_running_var)\n",
    "                \n",
    "                else:\n",
    "                    #Number of biases\n",
    "                    num_biases = conv.bias.numel()\n",
    "                \n",
    "                    #Load the weights\n",
    "                    conv_biases = torch.from_numpy(weights[ptr: ptr + num_biases])\n",
    "                    ptr = ptr + num_biases\n",
    "                \n",
    "                    #reshape the loaded weights according to the dims of the model weights\n",
    "                    conv_biases = conv_biases.view_as(conv.bias.data)\n",
    "                \n",
    "                    #Finally copy the data\n",
    "                    conv.bias.data.copy_(conv_biases)\n",
    "                    \n",
    "                #Let us load the weights for the Convolutional layers\n",
    "                num_weights = conv.weight.numel()\n",
    "                \n",
    "                #Do the same as above for weights\n",
    "                conv_weights = torch.from_numpy(weights[ptr:ptr+num_weights])\n",
    "                ptr = ptr + num_weights\n",
    "                \n",
    "                conv_weights = conv_weights.view_as(conv.weight.data)\n",
    "                conv.weight.data.copy_(conv_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d8017b-813c-48b6-b784-9c335208ae27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.py\n",
    "from __future__ import division\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import cv2 \n",
    "\n",
    "def unique(tensor):\n",
    "    tensor_np = tensor.cpu().numpy()\n",
    "    unique_np = np.unique(tensor_np)\n",
    "    unique_tensor = torch.from_numpy(unique_np)\n",
    "    \n",
    "    tensor_res = tensor.new(unique_tensor.shape)\n",
    "    tensor_res.copy_(unique_tensor)\n",
    "    return tensor_res\n",
    "\n",
    "\n",
    "def bbox_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Returns the IoU of two bounding boxes \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    #Get the coordinates of bounding boxes\n",
    "    b1_x1, b1_y1, b1_x2, b1_y2 = box1[:,0], box1[:,1], box1[:,2], box1[:,3]\n",
    "    b2_x1, b2_y1, b2_x2, b2_y2 = box2[:,0], box2[:,1], box2[:,2], box2[:,3]\n",
    "    \n",
    "    #get the corrdinates of the intersection rectangle\n",
    "    inter_rect_x1 =  torch.max(b1_x1, b2_x1)\n",
    "    inter_rect_y1 =  torch.max(b1_y1, b2_y1)\n",
    "    inter_rect_x2 =  torch.min(b1_x2, b2_x2)\n",
    "    inter_rect_y2 =  torch.min(b1_y2, b2_y2)\n",
    "    \n",
    "    #Intersection area\n",
    "    inter_area = torch.clamp(inter_rect_x2 - inter_rect_x1 + 1, min=0) * torch.clamp(inter_rect_y2 - inter_rect_y1 + 1, min=0)\n",
    "\n",
    "    #Union Area\n",
    "    b1_area = (b1_x2 - b1_x1 + 1)*(b1_y2 - b1_y1 + 1)\n",
    "    b2_area = (b2_x2 - b2_x1 + 1)*(b2_y2 - b2_y1 + 1)\n",
    "    \n",
    "    iou = inter_area / (b1_area + b2_area - inter_area)\n",
    "    \n",
    "    return iou\n",
    "\n",
    "def predict_transform(prediction, inp_dim, anchors, num_classes, CUDA=True):\n",
    "    \"\"\"Transform predictions from feature maps to bounding boxes\"\"\"\n",
    "    batch_size = prediction.size(0)\n",
    "    stride = inp_dim // prediction.size(2)\n",
    "    grid_size = inp_dim // stride\n",
    "    bbox_attrs = 5 + num_classes\n",
    "    num_anchors = len(anchors)\n",
    "    \n",
    "    # Reshape prediction to [batch_size, bbox_attrs * num_anchors, grid_size * grid_size]\n",
    "    prediction = prediction.view(batch_size, bbox_attrs * num_anchors, grid_size * grid_size)\n",
    "    prediction = prediction.transpose(1, 2).contiguous()\n",
    "    prediction = prediction.view(batch_size, grid_size * grid_size * num_anchors, bbox_attrs)\n",
    "    \n",
    "    # Scale anchors by stride\n",
    "    anchors = [(a[0]/stride, a[1]/stride) for a in anchors]\n",
    "    \n",
    "    # Sigmoid the center_X, center_Y. and object confidence\n",
    "    prediction[:, :, 0] = torch.sigmoid(prediction[:, :, 0])\n",
    "    prediction[:, :, 1] = torch.sigmoid(prediction[:, :, 1])\n",
    "    prediction[:, :, 4] = torch.sigmoid(prediction[:, :, 4])\n",
    "    \n",
    "    # Add the center offsets\n",
    "    device = prediction.device\n",
    "    grid = torch.arange(grid_size, device=device)\n",
    "    a, b = torch.meshgrid(grid, grid, indexing='ij')\n",
    "    x_offset = a.contiguous().view(-1, 1)\n",
    "    y_offset = b.contiguous().view(-1, 1)\n",
    "    \n",
    "    x_y_offset = torch.cat((x_offset, y_offset), 1).repeat(1, num_anchors).view(-1, 2).unsqueeze(0)\n",
    "    prediction[:, :, :2] += x_y_offset\n",
    "    \n",
    "    # Log space transform height and the width\n",
    "    anchors = torch.FloatTensor(anchors).to(device)\n",
    "    anchors = anchors.repeat(grid_size * grid_size, 1).unsqueeze(0)\n",
    "    prediction[:, :, 2:4] = torch.exp(prediction[:, :, 2:4]) * anchors\n",
    "    \n",
    "    # Sigmoid the class scores\n",
    "    prediction[:, :, 5:5+num_classes] = torch.sigmoid(prediction[:, :, 5:5+num_classes])\n",
    "    \n",
    "    # Resize the detection map to the input image size\n",
    "    prediction[:, :, :4] *= stride\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "def write_results(prediction, confidence, num_classes, nms_conf = 0.4):\n",
    "    conf_mask = (prediction[:,:,4] > confidence).float().unsqueeze(2)\n",
    "    prediction = prediction*conf_mask\n",
    "    \n",
    "    box_corner = prediction.new(prediction.shape)\n",
    "    box_corner[:,:,0] = (prediction[:,:,0] - prediction[:,:,2]/2)\n",
    "    box_corner[:,:,1] = (prediction[:,:,1] - prediction[:,:,3]/2)\n",
    "    box_corner[:,:,2] = (prediction[:,:,0] + prediction[:,:,2]/2) \n",
    "    box_corner[:,:,3] = (prediction[:,:,1] + prediction[:,:,3]/2)\n",
    "    prediction[:,:,:4] = box_corner[:,:,:4]\n",
    "    \n",
    "    batch_size = prediction.size(0)\n",
    "\n",
    "    write = False\n",
    "    \n",
    "\n",
    "\n",
    "    for ind in range(batch_size):\n",
    "        image_pred = prediction[ind]          #image Tensor\n",
    "       #confidence threshholding \n",
    "       #NMS\n",
    "    \n",
    "        max_conf, max_conf_score = torch.max(image_pred[:,5:5+ num_classes], 1)\n",
    "        max_conf = max_conf.float().unsqueeze(1)\n",
    "        max_conf_score = max_conf_score.float().unsqueeze(1)\n",
    "        seq = (image_pred[:,:5], max_conf, max_conf_score)\n",
    "        image_pred = torch.cat(seq, 1)\n",
    "        \n",
    "        non_zero_ind =  (torch.nonzero(image_pred[:,4]))\n",
    "        try:\n",
    "            image_pred_ = image_pred[non_zero_ind.squeeze(),:].view(-1,7)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        if image_pred_.shape[0] == 0:\n",
    "            continue       \n",
    "#        \n",
    "  \n",
    "        #Get the various classes detected in the image\n",
    "        img_classes = unique(image_pred_[:,-1])  # -1 index holds the class index\n",
    "        \n",
    "        \n",
    "        for cls in img_classes:\n",
    "            #perform NMS\n",
    "\n",
    "        \n",
    "            #get the detections with one particular class\n",
    "            cls_mask = image_pred_*(image_pred_[:,-1] == cls).float().unsqueeze(1)\n",
    "            class_mask_ind = torch.nonzero(cls_mask[:,-2]).squeeze()\n",
    "            image_pred_class = image_pred_[class_mask_ind].view(-1,7)\n",
    "            \n",
    "            #sort the detections such that the entry with the maximum objectness\n",
    "            #confidence is at the top\n",
    "            conf_sort_index = torch.sort(image_pred_class[:,4], descending = True )[1]\n",
    "            image_pred_class = image_pred_class[conf_sort_index]\n",
    "            idx = image_pred_class.size(0)   #Number of detections\n",
    "            \n",
    "            for i in range(idx):\n",
    "                #Get the IOUs of all boxes that come after the one we are looking at \n",
    "                #in the loop\n",
    "                try:\n",
    "                    ious = bbox_iou(image_pred_class[i].unsqueeze(0), image_pred_class[i+1:])\n",
    "                except ValueError:\n",
    "                    break\n",
    "            \n",
    "                except IndexError:\n",
    "                    break\n",
    "            \n",
    "                #Zero out all the detections that have IoU > treshhold\n",
    "                iou_mask = (ious < nms_conf).float().unsqueeze(1)\n",
    "                image_pred_class[i+1:] *= iou_mask       \n",
    "            \n",
    "                #Remove the non-zero entries\n",
    "                non_zero_ind = torch.nonzero(image_pred_class[:,4]).squeeze()\n",
    "                image_pred_class = image_pred_class[non_zero_ind].view(-1,7)\n",
    "                \n",
    "            batch_ind = image_pred_class.new(image_pred_class.size(0), 1).fill_(ind)      #Repeat the batch_id for as many detections of the class cls in the image\n",
    "            seq = batch_ind, image_pred_class\n",
    "            \n",
    "            if not write:\n",
    "                output = torch.cat(seq,1)\n",
    "                write = True\n",
    "            else:\n",
    "                out = torch.cat(seq,1)\n",
    "                output = torch.cat((output,out))\n",
    "\n",
    "    try:\n",
    "        return output\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "def letterbox_image(img, inp_dim):\n",
    "    '''resize image with unchanged aspect ratio using padding'''\n",
    "    img_w, img_h = img.shape[1], img.shape[0]\n",
    "    w, h = inp_dim\n",
    "    new_w = int(img_w * min(w/img_w, h/img_h))\n",
    "    new_h = int(img_h * min(w/img_w, h/img_h))\n",
    "    resized_image = cv2.resize(img, (new_w,new_h), interpolation = cv2.INTER_CUBIC)\n",
    "    \n",
    "    canvas = np.full((inp_dim[1], inp_dim[0], 3), 128)\n",
    "\n",
    "    canvas[(h-new_h)//2:(h-new_h)//2 + new_h,(w-new_w)//2:(w-new_w)//2 + new_w,  :] = resized_image\n",
    "    \n",
    "    return canvas\n",
    "\n",
    "def prep_image(img, inp_dim):\n",
    "    \"\"\"\n",
    "    Prepare image for inputting to the neural network. \n",
    "    \n",
    "    Returns a Variable \n",
    "    \"\"\"\n",
    "    img = (letterbox_image(img, (inp_dim, inp_dim)))\n",
    "    img = img[:,:,::-1].transpose((2,0,1)).copy()\n",
    "    img = torch.from_numpy(img).float().div(255.0).unsqueeze(0)\n",
    "    return img\n",
    "\n",
    "def load_classes(namesfile):\n",
    "    fp = open(namesfile, \"r\")\n",
    "    names = fp.read().split(\"\\n\")[:-1]\n",
    "    return names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93600aa9-6fd4-4e73-b1fe-4c6fb68dbc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yolov4.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def bbox_ciou(box1, box2, x1y1x2y2=True):\n",
    "    \"\"\"\n",
    "    Calculate CIoU loss between two bounding boxes\n",
    "    \"\"\"\n",
    "    if not x1y1x2y2:\n",
    "        # Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2]\n",
    "        b1_x1, b1_x2 = box1[:, 0] - box1[:, 2] / 2, box1[:, 0] + box1[:, 2] / 2\n",
    "        b1_y1, b1_y2 = box1[:, 1] - box1[:, 3] / 2, box1[:, 1] + box1[:, 3] / 2\n",
    "        b2_x1, b2_x2 = box2[:, 0] - box2[:, 2] / 2, box2[:, 0] + box2[:, 2] / 2\n",
    "        b2_y1, b2_y2 = box2[:, 1] - box2[:, 3] / 2, box2[:, 1] + box2[:, 3] / 2\n",
    "    else:\n",
    "        # Get the coordinates of bounding boxes\n",
    "        b1_x1, b1_y1, b1_x2, b1_y2 = box1[:, 0], box1[:, 1], box1[:, 2], box1[:, 3]\n",
    "        b2_x1, b2_y1, b2_x2, b2_y2 = box2[:, 0], box2[:, 1], box2[:, 2], box2[:, 3]\n",
    "\n",
    "    # Intersection area\n",
    "    inter = (torch.min(b1_x2, b2_x2) - torch.max(b1_x1, b2_x1)).clamp(0) * \\\n",
    "            (torch.min(b1_y2, b2_y2) - torch.max(b1_y1, b2_y1)).clamp(0)\n",
    "\n",
    "    # Union Area\n",
    "    w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1\n",
    "    w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1\n",
    "    union = w1 * h1 + w2 * h2 - inter + 1e-16\n",
    "\n",
    "    iou = inter / union\n",
    "\n",
    "    # Get enclosed coordinates\n",
    "    cw = torch.max(b1_x2, b2_x2) - torch.min(b1_x1, b2_x1)\n",
    "    ch = torch.max(b1_y2, b2_y2) - torch.min(b1_y1, b2_y1)\n",
    "\n",
    "    # Get diagonal distance\n",
    "    c2 = cw ** 2 + ch ** 2 + 1e-16\n",
    "\n",
    "    # Get center distance\n",
    "    rho2 = ((b2_x1 + b2_x2 - b1_x1 - b1_x2) ** 2 + \n",
    "            (b2_y1 + b2_y2 - b1_y1 - b1_y2) ** 2) / 4\n",
    "\n",
    "    # Calculate v and alpha\n",
    "    v = (4 / (math.pi ** 2)) * torch.pow(torch.atan(w2 / (h2 + 1e-16)) - torch.atan(w1 / (h1 + 1e-16)), 2)\n",
    "    with torch.no_grad():\n",
    "        alpha = v / (v - iou + (1 + 1e-16))\n",
    "\n",
    "    # CIoU\n",
    "    return iou - (rho2 / c2 + v * alpha)\n",
    "\n",
    "def xywh2xyxy(x):\n",
    "    # Convert bounding box format from [x, y, w, h] to [x1, y1, x2, y2]\n",
    "    y = x.clone()\n",
    "    y[:, 0] = x[:, 0] - x[:, 2] / 2\n",
    "    y[:, 1] = x[:, 1] - x[:, 3] / 2\n",
    "    y[:, 2] = x[:, 0] + x[:, 2] / 2\n",
    "    y[:, 3] = x[:, 1] + x[:, 3] / 2\n",
    "    return y\n",
    "\n",
    "class YOLOv4Loss(nn.Module):\n",
    "    def __init__(self, anchors, num_classes=80):\n",
    "        super(YOLOv4Loss, self).__init__()\n",
    "        self.anchors = anchors\n",
    "        self.num_classes = num_classes\n",
    "        self.ignore_thres = 0.5\n",
    "        self.obj_scale = 1\n",
    "        self.noobj_scale = 100\n",
    "        \n",
    "    def forward(self, predictions, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            predictions: tensor of shape (batch_size, num_boxes, 5 + num_classes)\n",
    "            targets: list of dicts containing boxes and labels\n",
    "        \"\"\"\n",
    "        device = predictions.device\n",
    "        batch_size = predictions.size(0)\n",
    "        total_loss = torch.tensor(0., requires_grad=True, device=device)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            pred = predictions[i]\n",
    "            target = targets[i]\n",
    "            \n",
    "            # Get target boxes and labels and move them to the correct device\n",
    "            target_boxes = target['boxes'].to(device)\n",
    "            target_labels = target['labels'].to(device)\n",
    "            \n",
    "            if len(target_boxes) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Get prediction components\n",
    "            pred_boxes = pred[..., :4]  # [x, y, w, h]\n",
    "            pred_conf = pred[..., 4]    # objectness\n",
    "            pred_cls = pred[..., 5:]    # class scores\n",
    "            \n",
    "            # Calculate IoU for each predicted box with each target box\n",
    "            num_pred = pred_boxes.size(0)\n",
    "            num_target = target_boxes.size(0)\n",
    "            \n",
    "            # Expand dimensions for broadcasting\n",
    "            pred_boxes = pred_boxes.unsqueeze(1).repeat(1, num_target, 1)\n",
    "            target_boxes = target_boxes.unsqueeze(0).repeat(num_pred, 1, 1)\n",
    "            \n",
    "            # Calculate CIoU loss\n",
    "            ciou = bbox_ciou(pred_boxes.view(-1, 4), target_boxes.view(-1, 4))\n",
    "            ciou = ciou.view(num_pred, num_target)\n",
    "            \n",
    "            # For each target, find the best matching prediction\n",
    "            best_ious, best_idx = ciou.max(dim=0)\n",
    "            \n",
    "            # Calculate box loss using CIoU\n",
    "            box_loss = (1.0 - best_ious).mean()\n",
    "            \n",
    "            # Calculate objectness loss\n",
    "            obj_mask = torch.zeros_like(pred_conf)\n",
    "            obj_mask[best_idx] = 1\n",
    "            obj_loss = F.binary_cross_entropy_with_logits(pred_conf, obj_mask)\n",
    "            \n",
    "            # Calculate classification loss\n",
    "            target_cls = torch.zeros_like(pred_cls)\n",
    "            for j, label in enumerate(target_labels):\n",
    "                target_cls[best_idx[j], label] = 1\n",
    "            cls_loss = F.binary_cross_entropy_with_logits(pred_cls, target_cls)\n",
    "            \n",
    "            # Combine losses\n",
    "            batch_loss = box_loss + obj_loss + cls_loss\n",
    "            total_loss = total_loss + batch_loss\n",
    "        \n",
    "        return total_loss / batch_size\n",
    "    \n",
    "    def __call__(self, predictions, targets):\n",
    "        return self.forward(predictions, targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15a5501-13a3-4abe-b11f-ab9cf57e23f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics.py\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "def compute_ap(recall, precision):\n",
    "    \"\"\"Compute the average precision, given the recall and precision curves.\"\"\"\n",
    "    # Append sentinel values to beginning and end\n",
    "    mrec = np.concatenate(([0.], recall, [1.]))\n",
    "    mpre = np.concatenate(([0.], precision, [0.]))\n",
    "\n",
    "    # Compute the precision envelope\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "    # Create a list of indexes where the recall changes\n",
    "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "\n",
    "    # Calculate the area under PR curve by sum of rectangular blocks\n",
    "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return ap\n",
    "\n",
    "def bbox_iou(box1, box2, x1y1x2y2=True):\n",
    "    \"\"\"Returns the IoU of two bounding boxes.\"\"\"\n",
    "    if not x1y1x2y2:\n",
    "        # Transform from center and width to exact coordinates\n",
    "        b1_x1, b1_x2 = box1[:, 0] - box1[:, 2] / 2, box1[:, 0] + box1[:, 2] / 2\n",
    "        b1_y1, b1_y2 = box1[:, 1] - box1[:, 3] / 2, box1[:, 1] + box1[:, 3] / 2\n",
    "        b2_x1, b2_x2 = box2[:, 0] - box2[:, 2] / 2, box2[:, 0] + box2[:, 2] / 2\n",
    "        b2_y1, b2_y2 = box2[:, 1] - box2[:, 3] / 2, box2[:, 1] + box2[:, 3] / 2\n",
    "    else:\n",
    "        # Get the coordinates of bounding boxes\n",
    "        b1_x1, b1_y1, b1_x2, b1_y2 = box1[:, 0], box1[:, 1], box1[:, 2], box1[:, 3]\n",
    "        b2_x1, b2_y1, b2_x2, b2_y2 = box2[:, 0], box2[:, 1], box2[:, 2], box2[:, 3]\n",
    "\n",
    "    # Get the coordinates of the intersection rectangle\n",
    "    inter_rect_x1 = torch.max(b1_x1, b2_x1)\n",
    "    inter_rect_y1 = torch.max(b1_y1, b2_y1)\n",
    "    inter_rect_x2 = torch.min(b1_x2, b2_x2)\n",
    "    inter_rect_y2 = torch.min(b1_y2, b2_y2)\n",
    "\n",
    "    # Intersection area\n",
    "    inter_area = torch.clamp(inter_rect_x2 - inter_rect_x1, 0) * torch.clamp(inter_rect_y2 - inter_rect_y1, 0)\n",
    "\n",
    "    # Union Area\n",
    "    b1_area = (b1_x2 - b1_x1) * (b1_y2 - b1_y1)\n",
    "    b2_area = (b2_x2 - b2_x1) * (b2_y2 - b2_y1)\n",
    "\n",
    "    return inter_area / (b1_area + b2_area - inter_area + 1e-16)\n",
    "\n",
    "def evaluate_coco_map(model, data_loader, coco_gt):\n",
    "    \"\"\"Evaluate mAP on COCO validation set\"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Prepare for COCO evaluation\n",
    "    coco_dt = []\n",
    "    image_ids = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, targets in data_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            batch_size = imgs.shape[0]\n",
    "            \n",
    "            # Forward pass with CUDA flag\n",
    "            predictions = model(imgs, device == torch.device(\"cuda\"))\n",
    "            \n",
    "            # Process predictions\n",
    "            for i in range(batch_size):\n",
    "                img_id = targets[i]['image_id'].item()\n",
    "                image_ids.append(img_id)\n",
    "                \n",
    "                if len(predictions[i]) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Convert predictions to COCO format\n",
    "                for pred in predictions[i]:\n",
    "                    x1, y1, x2, y2 = [p.item() for p in pred[:4]]\n",
    "                    conf = pred[4].item()\n",
    "                    cls_conf = pred[5].item()\n",
    "                    cls_pred = int(pred[6].item())  # Convert class prediction to integer\n",
    "                    \n",
    "                    coco_dt.append({\n",
    "                        'image_id': img_id,\n",
    "                        'category_id': cls_pred,\n",
    "                        'bbox': [float(x1), float(y1), float(x2 - x1), float(y2 - y1)],\n",
    "                        'score': float(conf * cls_conf)\n",
    "                    })\n",
    "    \n",
    "    if len(coco_dt) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Save predictions to temporary file\n",
    "    _, tmp_file = tempfile.mkstemp()\n",
    "    with open(tmp_file, 'w') as f:\n",
    "        json.dump(coco_dt, f)\n",
    "    \n",
    "    # Load predictions in COCO format\n",
    "    coco_pred = coco_gt.loadRes(tmp_file)\n",
    "    \n",
    "    # Run COCO evaluation\n",
    "    coco_eval = COCOeval(coco_gt, coco_pred, 'bbox')\n",
    "    coco_eval.params.imgIds = image_ids\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n",
    "    \n",
    "    return coco_eval.stats[0]  # Return mAP@[0.5:0.95]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d44d6c8-bab7-470d-8af1-886cfae7ca6a",
   "metadata": {},
   "source": [
    "### Task 2: Training\n",
    "\n",
    "1. Train the YOLOv4 model on the COCO dataset (or another dataset if you have one available).\n",
    "Here the purpose is not to get the best possible model (that would require implementing all\n",
    "of the \"bag of freebies\" training tricks described in the paper), but just some of them, to\n",
    "get a feel for their importance. - Done\n",
    "1. Get a set of ImageNet pretrained weights for CSPDarknet53 [from the Darknet GitHub repository](https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v4_pre/csdarknet53-omega_final.weights) - Done\n",
    "2. Add a method to load the pretrained weights into the backbone portion of your PyTorch YOLOv4 model. - Done\n",
    "3. Implement a basic `train_yolo` function similar to the `train_model` function you developed in previous\n",
    "   labs for classifiers that preprocesses the input with basic augmentation transformations, converts the\n",
    "   anchor-relative outputs to bounding box coordinates, computes MSE loss for the bounding box coordinates,\n",
    "   backpropagates the loss, and takes a step for the optimizer. Use the recommended IoU thresholds to determine\n",
    "   which predicted bounding boxes to include in the loss. You will find many examples of how to do this\n",
    "   online. - Done\n",
    "4. Train your model on COCO. Training on the full dataset to completion would take several days, so you can stop early after verifying\n",
    "   the model is learning in the first few epochs. - Done, trained only for 1 epoch\n",
    "5. Compute mAP for your model on the COCO validation set. - Done\n",
    "6. Implement the CIoU loss function and observe its effect on mAP. - Done, can be found under model/ciou.py file\n",
    "7. (Optional) Train on COCO to completion and see how close you can get to the mAP reported in the paper.\n",
    "\n",
    "\n",
    "I was using yolo pretrained weights with one epoch fine tune on coco dataset (due to time limitations), used 4 batch size based on gpu allowance. Samely, I trained from scratch only 1 epoch, the inference results of which are not good. All information regarding the training can be found in logs folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a369a67-946b-412d-8de5-dfadac3474da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # train_from_scratch.log\n",
    "\n",
    "# jupyter-st125457@puffer:~/rtml/a2_yolov4$ python train.py --data ./data/coco.yaml --batch-size 4\n",
    "# Loading YOLOv4 Model...\n",
    "# No pretrained weights found. Training from scratch...\n",
    "# Loading data configuration...\n",
    "\n",
    "# Creating datasets...\n",
    "# Training data: data/coco/val2017\n",
    "# Training annotations: data/coco/annotations/instances_val2017.json\n",
    "# Validation data: data/coco/val2017\n",
    "# Validation annotations: data/coco/annotations/instances_val2017.json\n",
    "# loading annotations into memory...\n",
    "# Done (t=0.49s)\n",
    "# creating index...\n",
    "# index created!\n",
    "# loading annotations into memory...\n",
    "# Done (t=0.51s)\n",
    "# creating index...\n",
    "# index created!\n",
    "# Dataset size: 5000 training images, 5000 validation images\n",
    "# loading annotations into memory...\n",
    "# Done (t=0.45s)\n",
    "# creating index...\n",
    "# index created!\n",
    "\n",
    "# Starting training...\n",
    "# Epoch: 0, Batch: 0, Loss: 2.4824\n",
    "# Epoch: 0, Batch: 10, Loss: 2.5740\n",
    "# Epoch: 0, Batch: 20, Loss: 2.5746\n",
    "# Epoch: 0, Batch: 30, Loss: 2.5707\n",
    "# Epoch: 0, Batch: 40, Loss: 2.5751\n",
    "# Epoch: 0, Batch: 50, Loss: 2.5773\n",
    "# Epoch: 0, Batch: 60, Loss: 2.5806\n",
    "# Epoch: 0, Batch: 70, Loss: 2.5834\n",
    "# Epoch: 0, Batch: 80, Loss: 2.5784\n",
    "# Epoch: 0, Batch: 90, Loss: 2.5810\n",
    "# Epoch: 0, Batch: 100, Loss: 2.5752\n",
    "# Epoch: 0, Batch: 110, Loss: 2.5788\n",
    "# Epoch: 0, Batch: 120, Loss: 2.5766\n",
    "# Epoch: 0, Batch: 130, Loss: 2.5735\n",
    "# Epoch: 0, Batch: 140, Loss: 2.5740\n",
    "# Epoch: 0, Batch: 150, Loss: 2.5723\n",
    "# Epoch: 0, Batch: 160, Loss: 2.5729\n",
    "# Epoch: 0, Batch: 170, Loss: 2.5702\n",
    "# Epoch: 0, Batch: 180, Loss: 2.5639\n",
    "# Epoch: 0, Batch: 190, Loss: 2.5637\n",
    "# Epoch: 0, Batch: 200, Loss: 2.5678\n",
    "# Epoch: 0, Batch: 210, Loss: 2.5691\n",
    "# Epoch: 0, Batch: 220, Loss: 2.5682\n",
    "# Epoch: 0, Batch: 230, Loss: 2.5680\n",
    "# Epoch: 0, Batch: 240, Loss: 2.5668\n",
    "# Epoch: 0, Batch: 250, Loss: 2.5660\n",
    "# Epoch: 0, Batch: 260, Loss: 2.5679\n",
    "# Epoch: 0, Batch: 270, Loss: 2.5664\n",
    "# Epoch: 0, Batch: 280, Loss: 2.5651\n",
    "# Epoch: 0, Batch: 290, Loss: 2.5668\n",
    "# Epoch: 0, Batch: 300, Loss: 2.5665\n",
    "# Epoch: 0, Batch: 310, Loss: 2.5661\n",
    "# Epoch: 0, Batch: 320, Loss: 2.5646\n",
    "# Epoch: 0, Batch: 330, Loss: 2.5626\n",
    "# Epoch: 0, Batch: 340, Loss: 2.5635\n",
    "# Epoch: 0, Batch: 350, Loss: 2.5646\n",
    "# Epoch: 0, Batch: 360, Loss: 2.5651\n",
    "# Epoch: 0, Batch: 370, Loss: 2.5667\n",
    "# Epoch: 0, Batch: 380, Loss: 2.5682\n",
    "# Epoch: 0, Batch: 390, Loss: 2.5681\n",
    "# Epoch: 0, Batch: 400, Loss: 2.5673\n",
    "# Epoch: 0, Batch: 410, Loss: 2.5671\n",
    "# Epoch: 0, Batch: 420, Loss: 2.5642\n",
    "# Epoch: 0, Batch: 430, Loss: 2.5632\n",
    "# Epoch: 0, Batch: 440, Loss: 2.5626\n",
    "# Epoch: 0, Batch: 450, Loss: 2.5623\n",
    "# Epoch: 0, Batch: 460, Loss: 2.5631\n",
    "# Epoch: 0, Batch: 470, Loss: 2.5619\n",
    "# Epoch: 0, Batch: 480, Loss: 2.5620\n",
    "# Epoch: 0, Batch: 490, Loss: 2.5628\n",
    "# Epoch: 0, Batch: 500, Loss: 2.5623\n",
    "# Epoch: 0, Batch: 510, Loss: 2.5630\n",
    "# Epoch: 0, Batch: 520, Loss: 2.5617\n",
    "# Epoch: 0, Batch: 530, Loss: 2.5620\n",
    "# Epoch: 0, Batch: 540, Loss: 2.5610\n",
    "# Epoch: 0, Batch: 550, Loss: 2.5609\n",
    "# Epoch: 0, Batch: 560, Loss: 2.5611\n",
    "# Epoch: 0, Batch: 570, Loss: 2.5622\n",
    "# Epoch: 0, Batch: 580, Loss: 2.5628\n",
    "# Epoch: 0, Batch: 590, Loss: 2.5631\n",
    "# Epoch: 0, Batch: 600, Loss: 2.5616\n",
    "# Epoch: 0, Batch: 610, Loss: 2.5613\n",
    "# Epoch: 0, Batch: 620, Loss: 2.5622\n",
    "# Epoch: 0, Batch: 630, Loss: 2.5628\n",
    "# Epoch: 0, Batch: 640, Loss: 2.5643\n",
    "# Epoch: 0, Batch: 650, Loss: 2.5650\n",
    "# Epoch: 0, Batch: 660, Loss: 2.5645\n",
    "# Epoch: 0, Batch: 670, Loss: 2.5628\n",
    "# Epoch: 0, Batch: 680, Loss: 2.5631\n",
    "# Epoch: 0, Batch: 690, Loss: 2.5633\n",
    "# Epoch: 0, Batch: 700, Loss: 2.5632\n",
    "# Epoch: 0, Batch: 710, Loss: 2.5623\n",
    "# Epoch: 0, Batch: 720, Loss: 2.5627\n",
    "# Epoch: 0, Batch: 730, Loss: 2.5627\n",
    "# Epoch: 0, Batch: 740, Loss: 2.5628\n",
    "# Epoch: 0, Batch: 750, Loss: 2.5633\n",
    "# Epoch: 0, Batch: 760, Loss: 2.5631\n",
    "# Epoch: 0, Batch: 770, Loss: 2.5633\n",
    "# Epoch: 0, Batch: 780, Loss: 2.5626\n",
    "# Epoch: 0, Batch: 790, Loss: 2.5614\n",
    "# Epoch: 0, Batch: 800, Loss: 2.5614\n",
    "# Epoch: 0, Batch: 810, Loss: 2.5612\n",
    "# Epoch: 0, Batch: 820, Loss: 2.5618\n",
    "# Epoch: 0, Batch: 830, Loss: 2.5619\n",
    "# Epoch: 0, Batch: 840, Loss: 2.5606\n",
    "# Epoch: 0, Batch: 850, Loss: 2.5610\n",
    "# Epoch: 0, Batch: 860, Loss: 2.5601\n",
    "# Epoch: 0, Batch: 870, Loss: 2.5600\n",
    "# Epoch: 0, Batch: 880, Loss: 2.5600\n",
    "# Epoch: 0, Batch: 890, Loss: 2.5605\n",
    "# Epoch: 0, Batch: 900, Loss: 2.5606\n",
    "# Epoch: 0, Batch: 910, Loss: 2.5601\n",
    "# Epoch: 0, Batch: 920, Loss: 2.5590\n",
    "# Epoch: 0, Batch: 930, Loss: 2.5601\n",
    "# Epoch: 0, Batch: 940, Loss: 2.5608\n",
    "# Epoch: 0, Batch: 950, Loss: 2.5608\n",
    "# Epoch: 0, Batch: 960, Loss: 2.5608\n",
    "# Epoch: 0, Batch: 970, Loss: 2.5605\n",
    "# Epoch: 0, Batch: 980, Loss: 2.5608\n",
    "# Epoch: 0, Batch: 990, Loss: 2.5604\n",
    "# Epoch: 0, Batch: 1000, Loss: 2.5607\n",
    "# Epoch: 0, Batch: 1010, Loss: 2.5610\n",
    "# Epoch: 0, Batch: 1020, Loss: 2.5609\n",
    "# Epoch: 0, Batch: 1030, Loss: 2.5605\n",
    "# Epoch: 0, Batch: 1040, Loss: 2.5598\n",
    "# Epoch: 0, Batch: 1050, Loss: 2.5607\n",
    "# Epoch: 0, Batch: 1060, Loss: 2.5611\n",
    "# Epoch: 0, Batch: 1070, Loss: 2.5610\n",
    "# Epoch: 0, Batch: 1080, Loss: 2.5608\n",
    "# Epoch: 0, Batch: 1090, Loss: 2.5598\n",
    "# Epoch: 0, Batch: 1100, Loss: 2.5600\n",
    "# Epoch: 0, Batch: 1110, Loss: 2.5602\n",
    "# Epoch: 0, Batch: 1120, Loss: 2.5603\n",
    "# Epoch: 0, Batch: 1130, Loss: 2.5599\n",
    "# Epoch: 0, Batch: 1140, Loss: 2.5597\n",
    "# Epoch: 0, Batch: 1150, Loss: 2.5604\n",
    "# Epoch: 0, Batch: 1160, Loss: 2.5607\n",
    "# Epoch: 0, Batch: 1170, Loss: 2.5612\n",
    "# Epoch: 0, Batch: 1180, Loss: 2.5612\n",
    "# Epoch: 0, Batch: 1190, Loss: 2.5607\n",
    "# Epoch: 0, Batch: 1200, Loss: 2.5608\n",
    "# Epoch: 0, Batch: 1210, Loss: 2.5614\n",
    "# Epoch: 0, Batch: 1220, Loss: 2.5616\n",
    "# Epoch: 0, Batch: 1230, Loss: 2.5614\n",
    "# Epoch: 0, Batch: 1240, Loss: 2.5613\n",
    "\n",
    "# Evaluating mAP...\n",
    "\n",
    "# Loading and preparing results...\n",
    "# DONE (t=7.37s)\n",
    "# creating index...\n",
    "# index created!\n",
    "# Running per image evaluation...\n",
    "# Evaluate annotation type *bbox*\n",
    "# DONE (t=2.92s).\n",
    "# Accumulating evaluation results...\n",
    "# DONE (t=0.11s).\n",
    "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
    "#  Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
    "#  Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
    "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
    "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
    "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
    "# Epoch 0 mAP: 0.0000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b275a410-ebf0-4982-907d-83b9e4671089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train_pretrained.log\n",
    "\n",
    "# jupyter-st125457@puffer:~/rtml/a2_yolov4$ python train.py --data ./data/coco.yaml --batch-size 4 --weights checkpoints/yolov4.weights\n",
    "# Loading YOLOv4 Model...\n",
    "# Loading weights from weights/yolov4.weights\n",
    "# Loading data configuration...\n",
    "\n",
    "# Creating datasets...\n",
    "# Training data: data/coco/val2017\n",
    "# Training annotations: data/coco/annotations/instances_val2017.json\n",
    "# Validation data: data/coco/val2017\n",
    "# Validation annotations: data/coco/annotations/instances_val2017.json\n",
    "# loading annotations into memory...\n",
    "# Done (t=0.47s)\n",
    "# creating index...\n",
    "# index created!\n",
    "# loading annotations into memory...\n",
    "# Done (t=0.46s)\n",
    "# creating index...\n",
    "# index created!\n",
    "# Dataset size: 5000 training images, 5000 validation images\n",
    "# loading annotations into memory...\n",
    "# Done (t=0.45s)\n",
    "# creating index...\n",
    "# index created!\n",
    "\n",
    "# Starting training...\n",
    "# Epoch: 0, Batch: 0, Loss: 2.2426\n",
    "# Epoch: 0, Batch: 10, Loss: 2.2749\n",
    "# Epoch: 0, Batch: 20, Loss: 2.2745\n",
    "# Epoch: 0, Batch: 30, Loss: 2.2340\n",
    "# Epoch: 0, Batch: 40, Loss: 2.2419\n",
    "# Epoch: 0, Batch: 50, Loss: 2.2415\n",
    "# Epoch: 0, Batch: 60, Loss: 2.2394\n",
    "# Epoch: 0, Batch: 70, Loss: 2.2434\n",
    "# Epoch: 0, Batch: 80, Loss: 2.2408\n",
    "# Epoch: 0, Batch: 90, Loss: 2.2423\n",
    "# Epoch: 0, Batch: 100, Loss: 2.2455\n",
    "# Epoch: 0, Batch: 110, Loss: 2.2412\n",
    "# Epoch: 0, Batch: 120, Loss: 2.2412\n",
    "# Epoch: 0, Batch: 130, Loss: 2.2417\n",
    "# Epoch: 0, Batch: 140, Loss: 2.2451\n",
    "# Epoch: 0, Batch: 150, Loss: 2.2478\n",
    "# Epoch: 0, Batch: 160, Loss: 2.2490\n",
    "# Epoch: 0, Batch: 170, Loss: 2.2484\n",
    "# Epoch: 0, Batch: 180, Loss: 2.2448\n",
    "# Epoch: 0, Batch: 190, Loss: 2.2456\n",
    "# Epoch: 0, Batch: 200, Loss: 2.2422\n",
    "# Epoch: 0, Batch: 210, Loss: 2.2430\n",
    "# Epoch: 0, Batch: 220, Loss: 2.2419\n",
    "# Epoch: 0, Batch: 230, Loss: 2.2410\n",
    "# Epoch: 0, Batch: 240, Loss: 2.2396\n",
    "# Epoch: 0, Batch: 250, Loss: 2.2406\n",
    "# Epoch: 0, Batch: 260, Loss: 2.2390\n",
    "# Epoch: 0, Batch: 270, Loss: 2.2382\n",
    "# Epoch: 0, Batch: 280, Loss: 2.2385\n",
    "# Epoch: 0, Batch: 290, Loss: 2.2383\n",
    "# Epoch: 0, Batch: 300, Loss: 2.2387\n",
    "# Epoch: 0, Batch: 310, Loss: 2.2378\n",
    "# Epoch: 0, Batch: 320, Loss: 2.2390\n",
    "# Epoch: 0, Batch: 330, Loss: 2.2384\n",
    "# Epoch: 0, Batch: 340, Loss: 2.2388\n",
    "# Epoch: 0, Batch: 350, Loss: 2.2392\n",
    "# Epoch: 0, Batch: 360, Loss: 2.2409\n",
    "# Epoch: 0, Batch: 370, Loss: 2.2418\n",
    "# Epoch: 0, Batch: 380, Loss: 2.2414\n",
    "# Epoch: 0, Batch: 390, Loss: 2.2419\n",
    "# Epoch: 0, Batch: 400, Loss: 2.2430\n",
    "# Epoch: 0, Batch: 410, Loss: 2.2438\n",
    "# Epoch: 0, Batch: 420, Loss: 2.2445\n",
    "# Epoch: 0, Batch: 430, Loss: 2.2446\n",
    "# Epoch: 0, Batch: 440, Loss: 2.2453\n",
    "# Epoch: 0, Batch: 450, Loss: 2.2456\n",
    "# Epoch: 0, Batch: 460, Loss: 2.2462\n",
    "# Epoch: 0, Batch: 470, Loss: 2.2467\n",
    "# Epoch: 0, Batch: 480, Loss: 2.2472\n",
    "# Epoch: 0, Batch: 490, Loss: 2.2481\n",
    "# Epoch: 0, Batch: 500, Loss: 2.2465\n",
    "# Epoch: 0, Batch: 510, Loss: 2.2463\n",
    "# Epoch: 0, Batch: 520, Loss: 2.2469\n",
    "# Epoch: 0, Batch: 530, Loss: 2.2470\n",
    "# Epoch: 0, Batch: 540, Loss: 2.2471\n",
    "# Epoch: 0, Batch: 550, Loss: 2.2469\n",
    "# Epoch: 0, Batch: 560, Loss: 2.2455\n",
    "# Epoch: 0, Batch: 570, Loss: 2.2462\n",
    "# Epoch: 0, Batch: 580, Loss: 2.2446\n",
    "# Epoch: 0, Batch: 590, Loss: 2.2453\n",
    "# Epoch: 0, Batch: 600, Loss: 2.2461\n",
    "# Epoch: 0, Batch: 610, Loss: 2.2452\n",
    "# Epoch: 0, Batch: 620, Loss: 2.2434\n",
    "# Epoch: 0, Batch: 630, Loss: 2.2432\n",
    "# Epoch: 0, Batch: 640, Loss: 2.2434\n",
    "# Epoch: 0, Batch: 650, Loss: 2.2437\n",
    "# Epoch: 0, Batch: 660, Loss: 2.2437\n",
    "# Epoch: 0, Batch: 670, Loss: 2.2436\n",
    "# Epoch: 0, Batch: 680, Loss: 2.2436\n",
    "# Epoch: 0, Batch: 690, Loss: 2.2434\n",
    "# Epoch: 0, Batch: 700, Loss: 2.2441\n",
    "# Epoch: 0, Batch: 710, Loss: 2.2445\n",
    "# Epoch: 0, Batch: 720, Loss: 2.2454\n",
    "# Epoch: 0, Batch: 730, Loss: 2.2455\n",
    "# Epoch: 0, Batch: 740, Loss: 2.2453\n",
    "# Epoch: 0, Batch: 750, Loss: 2.2441\n",
    "# Epoch: 0, Batch: 760, Loss: 2.2447\n",
    "# Epoch: 0, Batch: 770, Loss: 2.2449\n",
    "# Epoch: 0, Batch: 780, Loss: 2.2448\n",
    "# Epoch: 0, Batch: 790, Loss: 2.2440\n",
    "# Epoch: 0, Batch: 800, Loss: 2.2432\n",
    "# Epoch: 0, Batch: 810, Loss: 2.2431\n",
    "# Epoch: 0, Batch: 820, Loss: 2.2426\n",
    "# Epoch: 0, Batch: 830, Loss: 2.2418\n",
    "# Epoch: 0, Batch: 840, Loss: 2.2418\n",
    "# Epoch: 0, Batch: 850, Loss: 2.2423\n",
    "# Epoch: 0, Batch: 860, Loss: 2.2429\n",
    "# Epoch: 0, Batch: 870, Loss: 2.2424\n",
    "# Epoch: 0, Batch: 880, Loss: 2.2426\n",
    "# Epoch: 0, Batch: 890, Loss: 2.2429\n",
    "# Epoch: 0, Batch: 900, Loss: 2.2429\n",
    "# Epoch: 0, Batch: 910, Loss: 2.2428\n",
    "# Epoch: 0, Batch: 920, Loss: 2.2432\n",
    "# Epoch: 0, Batch: 930, Loss: 2.2420\n",
    "# Epoch: 0, Batch: 940, Loss: 2.2422\n",
    "# Epoch: 0, Batch: 950, Loss: 2.2424\n",
    "# Epoch: 0, Batch: 960, Loss: 2.2421\n",
    "# Epoch: 0, Batch: 970, Loss: 2.2426\n",
    "# Epoch: 0, Batch: 980, Loss: 2.2430\n",
    "# Epoch: 0, Batch: 990, Loss: 2.2435\n",
    "# Epoch: 0, Batch: 1000, Loss: 2.2431\n",
    "# Epoch: 0, Batch: 1010, Loss: 2.2430\n",
    "# Epoch: 0, Batch: 1020, Loss: 2.2433\n",
    "# Epoch: 0, Batch: 1030, Loss: 2.2440\n",
    "# Epoch: 0, Batch: 1040, Loss: 2.2443\n",
    "# Epoch: 0, Batch: 1050, Loss: 2.2447\n",
    "# Epoch: 0, Batch: 1060, Loss: 2.2443\n",
    "# Epoch: 0, Batch: 1070, Loss: 2.2446\n",
    "# Epoch: 0, Batch: 1080, Loss: 2.2448\n",
    "# Epoch: 0, Batch: 1090, Loss: 2.2450\n",
    "# Epoch: 0, Batch: 1100, Loss: 2.2454\n",
    "# Epoch: 0, Batch: 1110, Loss: 2.2452\n",
    "# Epoch: 0, Batch: 1120, Loss: 2.2449\n",
    "# Epoch: 0, Batch: 1130, Loss: 2.2453\n",
    "# Epoch: 0, Batch: 1140, Loss: 2.2451\n",
    "# Epoch: 0, Batch: 1150, Loss: 2.2455\n",
    "# Epoch: 0, Batch: 1160, Loss: 2.2453\n",
    "# Epoch: 0, Batch: 1170, Loss: 2.2452\n",
    "# Epoch: 0, Batch: 1180, Loss: 2.2451\n",
    "# Epoch: 0, Batch: 1190, Loss: 2.2452\n",
    "# Epoch: 0, Batch: 1200, Loss: 2.2453\n",
    "# Epoch: 0, Batch: 1210, Loss: 2.2458\n",
    "# Epoch: 0, Batch: 1220, Loss: 2.2462\n",
    "# Epoch: 0, Batch: 1230, Loss: 2.2462\n",
    "# Epoch: 0, Batch: 1240, Loss: 2.2464\n",
    "\n",
    "# Evaluating mAP..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69474a1e-10fa-470e-9f31-bfe95267acb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu_memory.txt\n",
    "\n",
    "# |   2  NVIDIA GeForce RTX 2080 Ti     Off |   00000000:88:00.0 Off |                  N/A |\n",
    "# | 32%   56C    P2            217W /  250W |    10144MiB /  11264MiB |     81%      Default |\n",
    "# |                                         |                        |                  N/A |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6405c7be-dfcc-4fa8-91ae-a7dbf0b1f089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # inference.txt\n",
    "\n",
    "# jupyter-st125457@puffer:~/rtml/a2_yolov4$ python detect.py --data data/coco.yaml --weights checkpoints/yolov4.weights --img cocoimages/000000521540.jpg \n",
    "# Loading YOLOv4 Model...\n",
    "# Detected 2 objects!\n",
    "# Class: spoon (44), Confidence: 0.8676, Box: [405.9, -22.0, 591.8, 220.0]\n",
    "# Class: banana (46), Confidence: 0.9979, Box: [132.3, -53.0, 520.3, 520.9]\n",
    "\n",
    "# Detection result saved to: /home/jupyter-st125457/rtml/a2_yolov4/results/result_000000521540.jpg\n",
    "\n",
    "# jupyter-st125457@puffer:~/rtml/a2_yolov4$ python run_yolov4.py \n",
    "# Loading network.....\n",
    "# Network successfully loaded\n",
    "# <class 'numpy.ndarray'> (478, 640, 3)\n",
    "# [ WARN:0@1.424] global loadsave.cpp:848 imwrite_ Unsupported depth image for selected encoder is fallbacked to CV_8U.\n",
    "# (608, 608, 3)\n",
    "# (608, 608, 3)\n",
    "# tensor([[608., 608., 608., 608.]])\n",
    "# 000000581781.jpg     predicted in  0.438 seconds\n",
    "# Objects Detected:    banana banana banana banana banana banana banana\n",
    "# ----------------------------------------------------------\n",
    "# Debug: c1=(285, 0), c2=(411, 114), type(c1)=<class 'tuple'>, type(c2)=<class 'tuple'>\n",
    "# Debug: img.shape=(608, 608, 3), type(img)=<class 'numpy.ndarray'>\n",
    "# Debug: c1=(127, 0), c2=(179, 108), type(c1)=<class 'tuple'>, type(c2)=<class 'tuple'>\n",
    "# Debug: img.shape=(608, 608, 3), type(img)=<class 'numpy.ndarray'>\n",
    "# Debug: c1=(135, 53), c2=(264, 179), type(c1)=<class 'tuple'>, type(c2)=<class 'tuple'>\n",
    "# Debug: img.shape=(608, 608, 3), type(img)=<class 'numpy.ndarray'>\n",
    "# Debug: c1=(343, 434), c2=(511, 608), type(c1)=<class 'tuple'>, type(c2)=<class 'tuple'>\n",
    "# Debug: img.shape=(608, 608, 3), type(img)=<class 'numpy.ndarray'>\n",
    "# Debug: c1=(318, 250), c2=(549, 346), type(c1)=<class 'tuple'>, type(c2)=<class 'tuple'>\n",
    "# Debug: img.shape=(608, 608, 3), type(img)=<class 'numpy.ndarray'>\n",
    "# Debug: c1=(188, 211), c2=(483, 334), type(c1)=<class 'tuple'>, type(c2)=<class 'tuple'>\n",
    "# Debug: img.shape=(608, 608, 3), type(img)=<class 'numpy.ndarray'>\n",
    "# Debug: c1=(141, 450), c2=(318, 608), type(c1)=<class 'tuple'>, type(c2)=<class 'tuple'>\n",
    "# Debug: img.shape=(608, 608, 3), type(img)=<class 'numpy.ndarray'>\n",
    "# SUMMARY\n",
    "# ----------------------------------------------------------\n",
    "# Task                     : Time Taken (in seconds)\n",
    "\n",
    "# Reading addresses        : 0.000\n",
    "# Loading batch            : 0.032\n",
    "# Detection (2 images)     : 0.537\n",
    "# Output Processing        : 0.000\n",
    "# Drawing Boxes            : 0.006\n",
    "# Average time_per_img     : 0.287\n",
    "\n",
    "# jupyter-st125457@puffer:~/rtml/a2_yolov4$ python detect.py --img data/coco/train2017/000000116031.jpg --weights checkpoints/yolov4.weights \n",
    "# Loading YOLOv4 Model...\n",
    "# Loading weights from checkpoints/yolov4.weights\n",
    "# Loading image: /home/jupyter-st125457/rtml/a2_yolov4/data/coco/train2017/000000116031.jpg\n",
    "# Found 2 objects!\n",
    "# Class: motorcycle (3), Confidence: 0.9774, Box: [-1.2, 5.8, 616.9, 598.8]\n",
    "# Class: cat (15), Confidence: 0.9938, Box: [280.3, 193.2, 540.8, 475.3]\n",
    "\n",
    "# Detection result saved to: /home/jupyter-st125457/rtml/a2_yolov4/results/result_000000116031.jpg\n",
    "# jupyter-st125457@puffer:~/rtml/a2_yolov4$ python detect.py --img data/coco/train2017/000000233141.jpg --weights checkpoints/yolov4.weights \n",
    "# Loading YOLOv4 Model...\n",
    "# Loading weights from checkpoints/yolov4.weights\n",
    "# Loading image: /home/jupyter-st125457/rtml/a2_yolov4/data/coco/train2017/000000233141.jpg\n",
    "# Found 2 objects!\n",
    "# Class: person (0), Confidence: 0.9992, Box: [313.3, 10.4, 443.1, 218.8]\n",
    "# Class: bench (13), Confidence: 0.9974, Box: [341.3, 20.7, 478.1, 197.9]\n",
    "\n",
    "# Detection result saved to: /home/jupyter-st125457/rtml/a2_yolov4/results/result_000000233141.jpg\n",
    "# jupyter-st125457@puffer:~/rtml/a2_yolov4$ python detect.py --img data/coco/train2017/000000523923.jpg --weights checkpoints/yolov4.weights \n",
    "# Loading YOLOv4 Model...\n",
    "# Loading weights from checkpoints/yolov4.weights\n",
    "# Loading image: /home/jupyter-st125457/rtml/a2_yolov4/data/coco/train2017/000000523923.jpg\n",
    "# Found 9 objects!\n",
    "# Class: person (0), Confidence: 0.9999, Box: [218.9, 97.3, 433.2, 489.6]\n",
    "# Class: person (0), Confidence: 0.9996, Box: [262.3, 460.5, 339.1, 618.2]\n",
    "# Class: person (0), Confidence: 0.9995, Box: [278.2, 312.2, 345.0, 507.8]\n",
    "# Class: person (0), Confidence: 0.9996, Box: [313.8, 501.8, 351.9, 637.8]\n",
    "# Class: person (0), Confidence: 0.9992, Box: [267.4, 395.8, 330.4, 558.8]\n",
    "# Class: person (0), Confidence: 0.9998, Box: [327.7, 486.5, 368.2, 620.6]\n",
    "# Class: person (0), Confidence: 0.9995, Box: [299.2, 364.6, 362.2, 535.1]\n",
    "# Class: person (0), Confidence: 0.9897, Box: [292.9, 433.4, 370.1, 595.6]\n",
    "# Class: skis (30), Confidence: 0.9985, Box: [459.8, 266.7, 579.4, 340.6]\n",
    "\n",
    "# Detection result saved to: /home/jupyter-st125457/rtml/a2_yolov4/results/result_000000523923.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfd5d83",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "The project establishes a strong groundwork for YOLOv4 object detection using CIoU loss with only single epoch training. Its modular design ensures easy maintenance and potential enhancements. Our implementation successfully integrates all key components, including model architecture, data pipeline, loss computation, and evaluation metrics. The code is well-structured, adhering to best practices by organizing functionalities into separate modules. Additionally, robust error handling and logging mechanisms were incorporated throughout the pipeline to facilitate smooth training and inference, providing clear feedback on potential issues.\n",
    "\n",
    "To validate the pipelines functionality and ensure the model was training correctly, I conducted a single epoch of training as a preliminary test.\n",
    "\n",
    "Trained models can be obtained via [link](https://drive.google.com/drive/folders/11ZdXXLB_ghn_kpEztK56SgFHrl_aId7q?usp=sharing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
